{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from config import *\n",
    "import glob\n",
    "from astropy.io import fits\n",
    "#import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 0\n",
      "1\n",
      "Real (0) = 55963 and Bogus (1) = 33479\n",
      "(268326, 51, 51)\n",
      "Final lenght of data = (89442, 51, 153)\n",
      "55963 33479\n",
      "33469 33489\n",
      "Len of data where len(ID_0) = len(ID_1) = 66958\n",
      "Final lenght of train = 46870, Final lenght of test = 20088 \n",
      "46870 20088\n",
      "Save train and test for 0\n",
      "46870 20088\n",
      "Save train and test targets for 0\n",
      "Save train and test IDs for 0\n",
      "[0 1] [10017 10071]\n",
      "[0 1] [23472 23398]\n",
      "Done with 0\n",
      "start 1\n",
      "1\n",
      "Real (0) = 54752 and Bogus (1) = 34690\n",
      "(268326, 51, 51)\n",
      "Final lenght of data = (89442, 51, 153)\n",
      "54752 34690\n",
      "34680 34700\n",
      "Len of data where len(ID_0) = len(ID_1) = 69380\n",
      "Final lenght of train = 48566, Final lenght of test = 20814 \n",
      "48566 20814\n",
      "Save train and test for 1\n",
      "48566 20814\n",
      "Save train and test targets for 1\n",
      "Save train and test IDs for 1\n",
      "[0 1] [10433 10381]\n",
      "[0 1] [24267 24299]\n",
      "Done with 1\n",
      "start 2\n",
      "1\n",
      "Real (0) = 49954 and Bogus (1) = 39488\n",
      "(268326, 51, 51)\n",
      "Final lenght of data = (89442, 51, 153)\n",
      "49954 39488\n",
      "39478 39498\n",
      "Len of data where len(ID_0) = len(ID_1) = 78976\n",
      "Final lenght of train = 55283, Final lenght of test = 23693 \n",
      "55283 23693\n",
      "Save train and test for 2\n",
      "55283 23693\n",
      "Save train and test targets for 2\n",
      "Save train and test IDs for 2\n",
      "[0 1] [11883 11810]\n",
      "[0 1] [27615 27668]\n",
      "Done with 2\n",
      "start 3\n",
      "1\n",
      "Real (0) = 51390 and Bogus (1) = 38052\n",
      "(268326, 51, 51)\n",
      "Final lenght of data = (89442, 51, 153)\n",
      "51390 38052\n",
      "38042 38062\n",
      "Len of data where len(ID_0) = len(ID_1) = 76104\n",
      "Final lenght of train = 53272, Final lenght of test = 22832 \n",
      "53272 22832\n",
      "Save train and test for 3\n",
      "53272 22832\n",
      "Save train and test targets for 3\n",
      "Save train and test IDs for 3\n",
      "[0 1] [11493 11339]\n",
      "[0 1] [26569 26703]\n",
      "Done with 3\n",
      "start 4\n",
      "1\n",
      "Real (0) = 45306 and Bogus (1) = 44136\n",
      "(268326, 51, 51)\n",
      "Final lenght of data = (89442, 51, 153)\n",
      "45306 44136\n",
      "44126 44146\n",
      "Len of data where len(ID_0) = len(ID_1) = 88272\n",
      "Final lenght of train = 61790, Final lenght of test = 26482 \n",
      "61790 26482\n",
      "Save train and test for 4\n",
      "61790 26482\n",
      "Save train and test targets for 4\n",
      "Save train and test IDs for 4\n",
      "[0 1] [13306 13176]\n",
      "[0 1] [30840 30950]\n",
      "Done with 4\n",
      "start 5\n",
      "1\n",
      "Real (0) = 54866 and Bogus (1) = 34576\n",
      "(268326, 51, 51)\n",
      "Final lenght of data = (89442, 51, 153)\n",
      "54866 34576\n",
      "34566 34586\n",
      "Len of data where len(ID_0) = len(ID_1) = 69152\n",
      "Final lenght of train = 48406, Final lenght of test = 20746 \n",
      "48406 20746\n",
      "Save train and test for 5\n",
      "48406 20746\n",
      "Save train and test targets for 5\n",
      "Save train and test IDs for 5\n",
      "[0 1] [10441 10305]\n",
      "[0 1] [24145 24261]\n",
      "Done with 5\n",
      "start 6\n",
      "1\n",
      "Real (0) = 36115 and Bogus (1) = 53327\n",
      "(268326, 51, 51)\n",
      "Final lenght of data = (89442, 51, 153)\n",
      "36115 53327\n",
      "36125 36105\n",
      "Len of data where len(ID_0) = len(ID_1) = 72230\n",
      "Final lenght of train = 50561, Final lenght of test = 21669 \n",
      "50561 21669\n",
      "Save train and test for 6\n",
      "50561 21669\n",
      "Save train and test targets for 6\n",
      "Save train and test IDs for 6\n",
      "[0 1] [10845 10824]\n",
      "[0 1] [25260 25301]\n",
      "Done with 6\n",
      "start 7\n",
      "1\n",
      "Real (0) = 39795 and Bogus (1) = 49647\n",
      "(268326, 51, 51)\n",
      "Final lenght of data = (89442, 51, 153)\n",
      "39795 49647\n",
      "39805 39785\n",
      "Len of data where len(ID_0) = len(ID_1) = 79590\n",
      "Final lenght of train = 55713, Final lenght of test = 23877 \n",
      "55713 23877\n",
      "Save train and test for 7\n",
      "55713 23877\n",
      "Save train and test targets for 7\n",
      "Save train and test IDs for 7\n",
      "[0 1] [11888 11989]\n",
      "[0 1] [27897 27816]\n",
      "Done with 7\n",
      "start 8\n",
      "1\n",
      "Real (0) = 44670 and Bogus (1) = 44766\n",
      "(268308, 51, 51)\n",
      "Final lenght of data = (89436, 51, 153)\n",
      "44670 44766\n",
      "44680 44660\n",
      "Len of data where len(ID_0) = len(ID_1) = 89340\n",
      "Final lenght of train = 62537, Final lenght of test = 26803 \n",
      "62537 26803\n",
      "Save train and test for 8\n",
      "62537 26803\n",
      "Save train and test targets for 8\n",
      "Save train and test IDs for 8\n",
      "[0 1] [13490 13313]\n",
      "[0 1] [31170 31367]\n",
      "Done with 8\n",
      "start 9\n",
      "1\n",
      "Real (0) = 21281 and Bogus (1) = 68155\n",
      "(268308, 51, 51)\n",
      "Final lenght of data = (89436, 51, 153)\n",
      "21281 68155\n",
      "21291 21271\n",
      "Len of data where len(ID_0) = len(ID_1) = 42562\n",
      "Final lenght of train = 29793, Final lenght of test = 12769 \n",
      "29793 12769\n",
      "Save train and test for 9\n",
      "29793 12769\n",
      "Save train and test targets for 9\n",
      "Save train and test IDs for 9\n",
      "[0 1] [6396 6373]\n",
      "[0 1] [14875 14918]\n",
      "Done with 9\n"
     ]
    }
   ],
   "source": [
    "# for ii in range(11):\n",
    "#      #Create path for diff, srch, temp images\n",
    "#     print('start {}'.format(ii))\n",
    "\n",
    "#     if ii != 10:\n",
    "#         path = os.path.join(configs[\"dpath\"],'stamps%d'%ii,'SNWG','Archive','*','Y1','*','*',pttype + '*.fits')\n",
    "#         flist.append(sorted(glob.glob(path)))\n",
    "#     else:\n",
    "#         path10 = os.path.join(configs[\"dpath\"],'stamps10','*',pttype + '*.fits')\n",
    "#         flist.append(sorted(glob.glob(path10)))                      \n",
    "#     print(len(flist))\n",
    "# for i in [\"20130829\",\"20130831\", \"20130901\"]:\n",
    "#     path = os.path.join(configs[\"dpath\"],'stamps1','SNWG','Archive','*','Y1',i,'*',pttype + '*.fits')\n",
    "#     flist.append(sorted(glob.glob(path)))\n",
    "\n",
    "pttype = '*'\n",
    "\n",
    "for ii in range(10):\n",
    "    flist = []\n",
    "    #Create path for diff, srch, temp images\n",
    "    print('start {}'.format(ii))\n",
    "    if ii != 10:\n",
    "        path = os.path.join(configs[\"dpath\"],'stamps%d'%ii,'SNWG','Archive','*','Y1','*','*',pttype + '*.fits')\n",
    "        flist.append(sorted(glob.glob(path)))\n",
    "    else:\n",
    "        path10 = os.path.join(configs[\"dpath\"],'stamps10','*',pttype + '*.fits')\n",
    "        flist.append(sorted(glob.glob(path10)))                      \n",
    "    print(len(flist))\n",
    "# for i in [\"20130829\",\"20130831\", \"20130901\"]:\n",
    "#     path = os.path.join(configs[\"dpath\"],'stamps1','SNWG','Archive','*','Y1',i,'*',pttype + '*.fits')\n",
    "#     flist.append(sorted(glob.glob(path)))\n",
    "\n",
    "    flist = np.concatenate((flist))\n",
    "    \n",
    "    ID =[int(f.split('/')[-1][4:-5]) for f in flist]\n",
    "\n",
    "    #extract from .feather file the ID that are on flist\n",
    "    ffpath = os.path.join(configs[\"dpath\"], \"autoscan_features.3.feather\") #this .feather file contain only the ID and OBJECT_TYPE for the images that I have on \n",
    "    new_labels = pd.read_feather(ffpath)\n",
    "    current_labels = new_labels[new_labels[\"ID\"].isin(ID)]\n",
    "    current_labels = current_labels[[\"ID\", \"OBJECT_TYPE\"]]\n",
    "    current_labels.drop_duplicates(inplace=True) \n",
    "    current_labels = current_labels.sort_values(by= [\"ID\"]).reset_index(drop=True)\n",
    "    counts_type = np.unique(current_labels['OBJECT_TYPE'], return_counts=True)\n",
    "    #how_many = {\"Real (0)\":counts_type[1][0], \"Bogus (1)\": counts_type[1][1] }\n",
    "\n",
    "    if len(counts_type[0]) == 2:\n",
    "        print(\"Real (0) = {} and Bogus (1) = {}\".format(counts_type[1][0], counts_type[1][1]))\n",
    "    if len(counts_type[0]) == 1:\n",
    "        if counts_type[0] == 0:\n",
    "            print(\"Real (0) = {}\".format(counts_type[1][0]))\n",
    "        else:\n",
    "            print(\"Bogus (1) = {}\".format(counts_type[1][0]))\n",
    "\n",
    "\n",
    "    imlist_dict = {}\n",
    "\n",
    "    # stores the name of the images as a list for ID above\n",
    "    #is a circle because i extract the ID for the flist, buttt\n",
    "    imlist_dict['flist'] = [f for f in flist if int(f.split('/')[-1][4:-5]) in current_labels['ID'].to_numpy()]\n",
    "    #print (len(imlist_dict['flist']))\n",
    "    #print(flist.nbytes)\n",
    "    #del(flist)\n",
    "    imlist_dict[\"imshp\"] = fits.open((imlist_dict[\"flist\"][0]))[0].data.shape #shape row,col\n",
    "    extension=\"fits\"\n",
    "    imdtype = {\"fits\":float, \"gif\":np.uint8, }\n",
    "\n",
    "    #sort as: descending ID and diff, srch, temp\n",
    "    imlist_dict[\"flist\"] = sorted(imlist_dict[\"flist\"], key=lambda s: s.split('/')[-1][:4])\n",
    "    imlist_dict[\"flist\"]= sorted(imlist_dict[\"flist\"], key=lambda s: int(s.split('/')[-1][4:-5]))\n",
    "\n",
    "    #container for data train and data test\n",
    "    data_full = np.zeros((len(imlist_dict[\"flist\"]),imlist_dict[\"imshp\"][0], imlist_dict[\"imshp\"][1]),imdtype[extension])\n",
    "\n",
    "    #fill the container and open images\n",
    "    for i in range(len(imlist_dict[\"flist\"])):\n",
    "        datas = fits.open(''.join(imlist_dict[\"flist\"][i]), memmap=True)\n",
    "        #datas.close()\n",
    "        data_full[i] = datas[0].data\n",
    "        #print(\"{}, path:{}\".format(i,imlist_dict[\"flist\"][i]))\n",
    "        datas.close()\n",
    "\n",
    "    print(data_full.shape)\n",
    "\n",
    "\n",
    "    data_norm = data_full.astype(float)\n",
    "    data_full = None\n",
    "    # # --normalize\n",
    "    # # mean and std for diff images\n",
    "    # # min and max for srch and temp\n",
    "\n",
    "    data_norm[::3] = (data_norm[::3]- data_norm[::3].mean(axis=(1,2), keepdims=True))/data_norm[::3].std(axis=(1,2), keepdims=True) #diff\n",
    "    data_norm[1::3]= (data_norm[1::3]-data_norm[1::3].min(axis=(1,2), keepdims=True))/(data_norm[1::3].max(axis=(1,2), keepdims=True)-data_norm[1::3].min(axis=(1,2), keepdims=True)) #srch\n",
    "    data_norm[2::3]= (data_norm[2::3]-data_norm[2::3].min(axis=(1,2), keepdims=True))/(data_norm[2::3].max(axis=(1,2), keepdims=True)-data_norm[2::3].min(axis=(1,2), keepdims=True)) #temp\n",
    "\n",
    "    #concatenate diff srch temp for the same ID\n",
    "\n",
    "    #final_data = np.zeros((int(len(data_full)//3),imlist_dict[\"imshp\"][0], imlist_dict[\"imshp\"][1]*3))\n",
    "    final_data = np.concatenate((data_norm[::3],data_norm[1::3],data_norm[2::3]), axis = 2)\n",
    "    data_norm = None\n",
    "    print('Final lenght of data = {}'.format(final_data.shape)) \n",
    "\n",
    "    #exxtract the objects  = 0\n",
    "    df_ID_0 = current_labels[current_labels[\"OBJECT_TYPE\"]==0]\n",
    "    #exxtract the objects  = 1\n",
    "    df_ID_1 = current_labels[current_labels[\"OBJECT_TYPE\"]==1]\n",
    "\n",
    "    #the len is the minimun of object 0, and object 1. To have equal data of both\n",
    "    len_each_set = min(len(df_ID_0), len(df_ID_1))\n",
    "    print(len(df_ID_0), len(df_ID_1))\n",
    "\n",
    "    if len_each_set != 0:\n",
    "        if len(df_ID_0) <= len_each_set:\n",
    "            #extract random the number of data classify as 0\n",
    "            index_data_ID0 = df_ID_0.sample(len_each_set-10, random_state = 2).sort_index()\n",
    "            #extract random the number of data classify as 1\n",
    "            index_data_ID1 = df_ID_1.sample(len_each_set+10,random_state = 2).sort_index()\n",
    "        else:\n",
    "            #extract random the number of data classify as 0\n",
    "            index_data_ID0 = df_ID_0.sample(len_each_set+10, random_state = 2).sort_index()\n",
    "            #extract random the number of data classify as 1\n",
    "            index_data_ID1 = df_ID_1.sample(len_each_set-10,random_state = 2).sort_index()\n",
    "        \n",
    "    if len(df_ID_0) == 0:\n",
    "        index_data_ID1 = df_ID_1.sort_index()\n",
    "        index_data_ID0 = df_ID_0\n",
    "        finalIDs = index_data_ID1\n",
    "        #index_data_ID1.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "    if len(df_ID_1) == 0:\n",
    "        index_data_ID0 = df_ID_0.sort_index()\n",
    "        index_data_ID1 = df_ID_1\n",
    "        finalIDs = index_data_ID0\n",
    "        #index_data_ID0.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "\n",
    "    finalIDs = index_data_ID0.append(index_data_ID1)\n",
    "    #finalIDs.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "\n",
    "    print(len(index_data_ID1),len(index_data_ID0))\n",
    "\n",
    "    #convert index to numpy to iterate\n",
    "    index_ID0 = index_data_ID0.index.to_numpy()\n",
    "\n",
    "    #convert index to numpy to iterate\n",
    "    index_ID1 = index_data_ID1.index.to_numpy()\n",
    "\n",
    "    #concatenate both index\n",
    "    indexes = sorted(np.concatenate((index_ID0, index_ID1)))\n",
    "\n",
    "    #extract the data from the index given above, of the complete data, where 0 and 1 are not equal\n",
    "    equal_type_data = len(indexes)\n",
    "    print(\"Len of data where len(ID_0) = len(ID_1) = {}\".format(equal_type_data))\n",
    "\n",
    "    #75% is for training\n",
    "    #25% testing\n",
    "    train_len = int(equal_type_data*0.70)\n",
    "    test_len = equal_type_data  - int(equal_type_data*0.70)\n",
    "    print('Final lenght of train = {}, Final lenght of test = {} '.format(train_len, test_len))\n",
    "\n",
    "    import random\n",
    "    random.seed(4)\n",
    "    random_index = random.sample(range(0, equal_type_data), train_len)\n",
    "\n",
    "    train = np.array([final_data[i] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "    test = np.array([final_data[i] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "    \n",
    "    print(len(train),len(test))\n",
    "    np.save('../data/data_split_n/train%d'%ii+'.npy', train)\n",
    "    np.save('../data/data_split_n/test%d'%ii+'.npy', test)\n",
    "    print('Save train and test for {}'.format(ii))\n",
    "\n",
    "\n",
    "    # #extracting the label 0 or 1\n",
    "    targets = [current_labels.iloc[i][\"OBJECT_TYPE\"] for i in indexes]\n",
    "\n",
    "    #split the targets\n",
    "    train_targ = np.array([current_labels.iloc[i][\"OBJECT_TYPE\"] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "    test_targ = np.array([current_labels.iloc[i][\"OBJECT_TYPE\"] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "\n",
    "    train_ID = np.array([current_labels.iloc[i][\"ID\"] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "    test_ID = np.array([current_labels.iloc[i][\"ID\"] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "    print(len(train_ID),len(test_ID))\n",
    "\n",
    "    np.save('../data/data_split_n/train_targ_%d'%ii+'.npy', train_targ)\n",
    "    np.save('../data/data_split_n/test_targ_%d'%ii+'.npy', test_targ)\n",
    "    print('Save train and test targets for {}'.format(ii))\n",
    "\n",
    "    np.save('../data/data_split_n/train_ID_%d'%ii+'.npy', train_ID)\n",
    "    np.save('../data/data_split_n/test_ID_%d'%ii+'.npy', test_ID)\n",
    "    print('Save train and test IDs for {}'.format(ii))\n",
    "\n",
    "    (unique, counts) = np.unique(test_targ, return_counts=True)\n",
    "    print(unique, counts)\n",
    "\n",
    "    (unique, counts) = np.unique(train_targ, return_counts=True)\n",
    "    print(unique, counts)\n",
    "    \n",
    "    print('Done with {}'.format(ii))\n",
    "    \n",
    "    flist = None\n",
    "    final_data = None\n",
    "    train = None\n",
    "    test = None\n",
    "    imlist_dict = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in range(1,2):\n",
    "#     pttype = '*'\n",
    "#     flist = []\n",
    "#     print('start {}'.format(ii))\n",
    "#     path = os.path.join(configs[\"dpath\"],'stamps%d'%ii,'SNWG','Archive','*','Y1','*','*',pttype + '*.fits')\n",
    "#     flist.append(sorted(glob.glob(path)))\n",
    "#     flist = np.concatenate((flist))\n",
    "\n",
    "#     #Extract the ID of the flist\n",
    "#     ID =[int(f.split('/')[-1][4:-5]) for f in flist]\n",
    "\n",
    "#     #extract from .feather file the ID that are on flist\n",
    "#     ffpath = os.path.join(configs[\"dpath\"], \"autoscan_features.3.feather\") #this .feather file contain only the ID and OBJECT_TYPE for the images that I have on \n",
    "#     new_labels = pd.read_feather(ffpath)\n",
    "#     current_labels = new_labels[new_labels[\"ID\"].isin(ID)]\n",
    "#     current_labels = current_labels[[\"ID\", \"OBJECT_TYPE\"]]\n",
    "#     current_labels.drop_duplicates(inplace=True) \n",
    "#     current_labels = current_labels.sort_values(by= [\"ID\"]).reset_index(drop=True)\n",
    "#     counts_type = np.unique(current_labels['OBJECT_TYPE'], return_counts=True)\n",
    "#     #how_many = {\"Real (0)\":counts_type[1][0], \"Bogus (1)\": counts_type[1][1] }\n",
    "\n",
    "#     if len(counts_type[0]) == 2:\n",
    "#         print(\"Real (0) = {} and Bogus (1) = {}\".format(counts_type[1][0], counts_type[1][1]))\n",
    "#     if len(counts_type[0]) == 1:\n",
    "#         if counts_type[0] == 0:\n",
    "#             print(\"Real (0) = {}\".format(counts_type[1][0]))\n",
    "#         else:\n",
    "#             print(\"Bogus (1) = {}\".format(counts_type[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 4\n",
      "1\n",
      "Real (0) = 45306 and Bogus (1) = 44136\n",
      "45306 44136\n",
      "44126 44146\n",
      "Len of data where len(ID_0) = len(ID_1) = 88272\n",
      "Final lenght of train = 61790, Final lenght of test = 26482 \n",
      "61790 26482\n",
      "Save train and test targets for 4\n",
      "Save train and test IDs for 4\n",
      "[0 1] [13306 13176]\n",
      "[0 1] [30840 30950]\n",
      "Done with 4\n"
     ]
    }
   ],
   "source": [
    "pttype = '*'\n",
    "\n",
    "for ii in range(4,5):\n",
    "    flist = []\n",
    "    #Create path for diff, srch, temp images\n",
    "    print('start {}'.format(ii))\n",
    "    if ii != 10:\n",
    "        path = os.path.join(configs[\"dpath\"],'stamps%d'%ii,'SNWG','Archive','*','Y1','*','*',pttype + '*.fits')\n",
    "        flist.append(sorted(glob.glob(path)))\n",
    "    else:\n",
    "        path10 = os.path.join(configs[\"dpath\"],'stamps10','*',pttype + '*.fits')\n",
    "        flist.append(sorted(glob.glob(path10)))                      \n",
    "    print(len(flist))\n",
    "# for i in [\"20130829\",\"20130831\", \"20130901\"]:\n",
    "#     path = os.path.join(configs[\"dpath\"],'stamps1','SNWG','Archive','*','Y1',i,'*',pttype + '*.fits')\n",
    "#     flist.append(sorted(glob.glob(path)))\n",
    "\n",
    "    flist = np.concatenate((flist))\n",
    "    \n",
    "    ID =[int(f.split('/')[-1][4:-5]) for f in flist]\n",
    "\n",
    "    #extract from .feather file the ID that are on flist\n",
    "    ffpath = os.path.join(configs[\"dpath\"], \"autoscan_features.3.feather\") #this .feather file contain only the ID and OBJECT_TYPE for the images that I have on \n",
    "    new_labels = pd.read_feather(ffpath)\n",
    "    current_labels = new_labels[new_labels[\"ID\"].isin(ID)]\n",
    "    current_labels = current_labels[[\"ID\", \"OBJECT_TYPE\"]]\n",
    "    current_labels.drop_duplicates(inplace=True) \n",
    "    current_labels = current_labels.sort_values(by= [\"ID\"]).reset_index(drop=True)\n",
    "    counts_type = np.unique(current_labels['OBJECT_TYPE'], return_counts=True)\n",
    "    #how_many = {\"Real (0)\":counts_type[1][0], \"Bogus (1)\": counts_type[1][1] }\n",
    "\n",
    "    if len(counts_type[0]) == 2:\n",
    "        print(\"Real (0) = {} and Bogus (1) = {}\".format(counts_type[1][0], counts_type[1][1]))\n",
    "    if len(counts_type[0]) == 1:\n",
    "        if counts_type[0] == 0:\n",
    "            print(\"Real (0) = {}\".format(counts_type[1][0]))\n",
    "        else:\n",
    "            print(\"Bogus (1) = {}\".format(counts_type[1][0]))\n",
    "\n",
    "    #exxtract the objects  = 0\n",
    "    df_ID_0 = current_labels[current_labels[\"OBJECT_TYPE\"]==0]\n",
    "    #exxtract the objects  = 1\n",
    "    df_ID_1 = current_labels[current_labels[\"OBJECT_TYPE\"]==1]\n",
    "\n",
    "    #the len is the minimun of object 0, and object 1. To have equal data of both\n",
    "    len_each_set = min(len(df_ID_0), len(df_ID_1))\n",
    "    print(len(df_ID_0), len(df_ID_1))\n",
    "\n",
    "    if len_each_set != 0:\n",
    "        if len(df_ID_0) <= len_each_set:\n",
    "            #extract random the number of data classify as 0\n",
    "            index_data_ID0 = df_ID_0.sample(len_each_set-10, random_state = 2).sort_index()\n",
    "            #extract random the number of data classify as 1\n",
    "            index_data_ID1 = df_ID_1.sample(len_each_set+10,random_state = 2).sort_index()\n",
    "        else:\n",
    "            #extract random the number of data classify as 0\n",
    "            index_data_ID0 = df_ID_0.sample(len_each_set+10, random_state = 2).sort_index()\n",
    "            #extract random the number of data classify as 1\n",
    "            index_data_ID1 = df_ID_1.sample(len_each_set-10,random_state = 2).sort_index()\n",
    "        \n",
    "    if len(df_ID_0) == 0:\n",
    "        index_data_ID1 = df_ID_1.sort_index()\n",
    "        index_data_ID0 = df_ID_0\n",
    "        finalIDs = index_data_ID1\n",
    "        #index_data_ID1.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "    if len(df_ID_1) == 0:\n",
    "        index_data_ID0 = df_ID_0.sort_index()\n",
    "        index_data_ID1 = df_ID_1\n",
    "        finalIDs = index_data_ID0\n",
    "        #index_data_ID0.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "\n",
    "    finalIDs = index_data_ID0.append(index_data_ID1)\n",
    "    #finalIDs.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "\n",
    "    print(len(index_data_ID1),len(index_data_ID0))\n",
    "\n",
    "    #convert index to numpy to iterate\n",
    "    index_ID0 = index_data_ID0.index.to_numpy()\n",
    "\n",
    "    #convert index to numpy to iterate\n",
    "    index_ID1 = index_data_ID1.index.to_numpy()\n",
    "\n",
    "    #concatenate both index\n",
    "    indexes = sorted(np.concatenate((index_ID0, index_ID1)))\n",
    "\n",
    "    #extract the data from the index given above, of the complete data, where 0 and 1 are not equal\n",
    "    equal_type_data = len(indexes)\n",
    "    print(\"Len of data where len(ID_0) = len(ID_1) = {}\".format(equal_type_data))\n",
    "\n",
    "    #75% is for training\n",
    "    #25% testing\n",
    "    train_len = int(equal_type_data*0.70)\n",
    "    test_len = equal_type_data  - int(equal_type_data*0.70)\n",
    "    print('Final lenght of train = {}, Final lenght of test = {} '.format(train_len, test_len))\n",
    "\n",
    "    import random\n",
    "    random.seed(4)\n",
    "    random_index = random.sample(range(0, equal_type_data), train_len)\n",
    "\n",
    "    # #extracting the label 0 or 1\n",
    "    targets = [current_labels.iloc[i][\"OBJECT_TYPE\"] for i in indexes]\n",
    "\n",
    "    #split the targets\n",
    "    train_targ = np.array([current_labels.iloc[i][\"OBJECT_TYPE\"] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "    test_targ = np.array([current_labels.iloc[i][\"OBJECT_TYPE\"] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "\n",
    "    train_ID = np.array([current_labels.iloc[i][\"ID\"] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "    test_ID = np.array([current_labels.iloc[i][\"ID\"] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "    print(len(train_ID),len(test_ID))\n",
    "\n",
    "    np.save('../data/data_split_n/train_targ_%d'%ii+'.npy', train_targ)\n",
    "    np.save('../data/data_split_n/test_targ_%d'%ii+'.npy', test_targ)\n",
    "    print('Save train and test targets for {}'.format(ii))\n",
    "\n",
    "    np.save('../data/data_split_n/train_ID_%d'%ii+'.npy', train_ID)\n",
    "    np.save('../data/data_split_n/test_ID_%d'%ii+'.npy', test_ID)\n",
    "    print('Save train and test IDs for {}'.format(ii))\n",
    "\n",
    "    (unique, counts) = np.unique(test_targ, return_counts=True)\n",
    "    print(unique, counts)\n",
    "\n",
    "    (unique, counts) = np.unique(train_targ, return_counts=True)\n",
    "    print(unique, counts)\n",
    "    \n",
    "    print('Done with {}'.format(ii))\n",
    "    \n",
    "    flist = None\n",
    "    final_data = None\n",
    "    train = None\n",
    "    test = None\n",
    "    imlist_dict = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tff-TAC",
   "language": "python",
   "name": "tff-tac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
